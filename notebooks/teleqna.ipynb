{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37ca0ed0-c01d-402b-9bc0-d1578c646e75",
   "metadata": {},
   "source": [
    "# TeleQnA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd22152b-ca99-4688-a5f0-6e3be6a4c4cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Create vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "553a16dc-4d7c-4a96-b3b7-9b460a1e0077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model embedding\n",
      "Using device: cuda\n",
      "Total number of documents: 554\n",
      "Creating passages\n",
      "  5%|â–ˆâ–‰                                        | 26/554 [00:25<17:08,  1.95s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (832 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 554/554 [11:00<00:00,  1.19s/it]\n",
      "Removing duplicate passages\n",
      "Total number of passages created: 378571\n",
      "Creating vector store\n",
      "Generando embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 185/185 [05:32<00:00,  1.80s/it]\n",
      "âœ… Ãndice FAISS creado exitosamente.\n",
      "ðŸ’¾ Vector store guardado en ../vector_stores/teleqna/base_vector_store_teleqna\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/create_vector_store.py \\\n",
    "  --dataset \"teleqna\" \\\n",
    "  --emb_model \"BAAI/bge-small-en-v1.5\" \\\n",
    "  --cs 150 \\\n",
    "  --co 20 \\\n",
    "  --bs_emb 2048 \\\n",
    "  --output_dir \"../vector_stores/teleqna/base_\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2982078-9bc8-4904-8654-ec6bb9db8b4b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2420ab28-8ec2-4061-959c-48506a268295",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdinho15971\u001b[0m (\u001b[33mdinho15971-unicamp\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "Starting main process...\n",
      "Loading and preparing datasets...\n",
      "Datasets loaded and prepared.\n",
      "Creating evaluator...\n",
      "Evaluator created.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/local1/ronaldinho/projects/FinalSBBD/notebooks/wandb/run-20250427_021758-fgc0qdb6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbge-small-3gpp_10e_128bs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/dinho15971-unicamp/SBBD_embeddings\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dinho15971-unicamp/SBBD_embeddings/runs/fgc0qdb6\u001b[0m\n",
      "Loading model: BAAI/bge-small-en-v1.5\n",
      "Model and loss function initialized.\n",
      "Configuring training arguments...\n",
      "Training arguments configured.\n",
      "Starting training process...\n",
      "{'loss': 0.3891, 'grad_norm': 1.2274510860443115, 'learning_rate': 4.734081600808531e-05, 'epoch': 2.5}\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 15/60 [00:02<00:04,  9.48it/s]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.06870950758457184, 'eval_telecom-ir-eval_cosine_accuracy@1': 0.9834254143646409, 'eval_telecom-ir-eval_cosine_accuracy@3': 0.994475138121547, 'eval_telecom-ir-eval_cosine_accuracy@5': 1.0, 'eval_telecom-ir-eval_cosine_accuracy@10': 1.0, 'eval_telecom-ir-eval_cosine_precision@1': 0.9834254143646409, 'eval_telecom-ir-eval_cosine_recall@1': 0.9834254143646409, 'eval_telecom-ir-eval_cosine_ndcg@10': 0.9927764423492613, 'eval_telecom-ir-eval_cosine_mrr@3': 0.988950276243094, 'eval_telecom-ir-eval_cosine_mrr@5': 0.9903314917127072, 'eval_telecom-ir-eval_cosine_mrr@10': 0.9903314917127072, 'eval_telecom-ir-eval_cosine_map@100': 0.9903314917127072, 'eval_runtime': 0.2964, 'eval_samples_per_second': 610.673, 'eval_steps_per_second': 6.748, 'epoch': 2.5}\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 15/60 [00:02<00:04,  9.48it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.76it/s]\u001b[A\n",
      "{'loss': 0.0459, 'grad_norm': 0.2939002513885498, 'learning_rate': 3.076539676856101e-05, 'epoch': 5.0}\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                     | 30/60 [00:04<00:03,  8.42it/s]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.052123747766017914, 'eval_telecom-ir-eval_cosine_accuracy@1': 0.988950276243094, 'eval_telecom-ir-eval_cosine_accuracy@3': 0.994475138121547, 'eval_telecom-ir-eval_cosine_accuracy@5': 0.994475138121547, 'eval_telecom-ir-eval_cosine_accuracy@10': 1.0, 'eval_telecom-ir-eval_cosine_precision@1': 0.988950276243094, 'eval_telecom-ir-eval_cosine_recall@1': 0.988950276243094, 'eval_telecom-ir-eval_cosine_ndcg@10': 0.9944040714954667, 'eval_telecom-ir-eval_cosine_mrr@3': 0.9917127071823204, 'eval_telecom-ir-eval_cosine_mrr@5': 0.9917127071823204, 'eval_telecom-ir-eval_cosine_mrr@10': 0.9926335174953961, 'eval_telecom-ir-eval_cosine_map@100': 0.9926335174953959, 'eval_runtime': 0.29, 'eval_samples_per_second': 624.111, 'eval_steps_per_second': 6.896, 'epoch': 5.0}\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                     | 30/60 [00:04<00:03,  8.42it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.95it/s]\u001b[A\n",
      "{'loss': 0.0246, 'grad_norm': 0.2363736778497696, 'learning_rate': 1.0071035207430352e-05, 'epoch': 7.5}\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž          | 45/60 [00:06<00:01,  8.31it/s]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.05150328204035759, 'eval_telecom-ir-eval_cosine_accuracy@1': 0.988950276243094, 'eval_telecom-ir-eval_cosine_accuracy@3': 0.994475138121547, 'eval_telecom-ir-eval_cosine_accuracy@5': 0.994475138121547, 'eval_telecom-ir-eval_cosine_accuracy@10': 1.0, 'eval_telecom-ir-eval_cosine_precision@1': 0.988950276243094, 'eval_telecom-ir-eval_cosine_recall@1': 0.988950276243094, 'eval_telecom-ir-eval_cosine_ndcg@10': 0.9944040714954667, 'eval_telecom-ir-eval_cosine_mrr@3': 0.9917127071823204, 'eval_telecom-ir-eval_cosine_mrr@5': 0.9917127071823204, 'eval_telecom-ir-eval_cosine_mrr@10': 0.9926335174953961, 'eval_telecom-ir-eval_cosine_map@100': 0.9926335174953959, 'eval_runtime': 0.2963, 'eval_samples_per_second': 610.88, 'eval_steps_per_second': 6.75, 'epoch': 7.5}\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž          | 45/60 [00:07<00:01,  8.31it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.77it/s]\u001b[A\n",
      "{'loss': 0.0158, 'grad_norm': 0.1372489035129547, 'learning_rate': 4.229604321829561e-08, 'epoch': 10.0}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:09<00:00,  9.35it/s]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.052012063562870026, 'eval_telecom-ir-eval_cosine_accuracy@1': 0.988950276243094, 'eval_telecom-ir-eval_cosine_accuracy@3': 0.994475138121547, 'eval_telecom-ir-eval_cosine_accuracy@5': 0.994475138121547, 'eval_telecom-ir-eval_cosine_accuracy@10': 1.0, 'eval_telecom-ir-eval_cosine_precision@1': 0.988950276243094, 'eval_telecom-ir-eval_cosine_recall@1': 0.988950276243094, 'eval_telecom-ir-eval_cosine_ndcg@10': 0.9944040714954667, 'eval_telecom-ir-eval_cosine_mrr@3': 0.9917127071823204, 'eval_telecom-ir-eval_cosine_mrr@5': 0.9917127071823204, 'eval_telecom-ir-eval_cosine_mrr@10': 0.9926335174953961, 'eval_telecom-ir-eval_cosine_map@100': 0.9926335174953959, 'eval_runtime': 0.2908, 'eval_samples_per_second': 622.323, 'eval_steps_per_second': 6.876, 'epoch': 10.0}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:09<00:00,  9.35it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.92it/s]\u001b[A\n",
      "{'train_runtime': 10.0702, 'train_samples_per_second': 718.956, 'train_steps_per_second': 5.958, 'train_loss': 0.11883130272229513, 'epoch': 10.0}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:10<00:00,  5.94it/s]\n",
      "Training completed.\n",
      "Saving the trained model...\n",
      "Model saved successfully.\n",
      "Process completed successfully.\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: ðŸš€ View run \u001b[33mbge-small-3gpp_10e_128bs\u001b[0m at: \u001b[34mhttps://wandb.ai/dinho15971-unicamp/SBBD_embeddings/runs/fgc0qdb6\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250427_021758-fgc0qdb6/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python  ../scripts/train_embedding.py \\\n",
    "  --name_dataset \"teleqna\" \\\n",
    "  --model_name \"BAAI/bge-small-en-v1.5\" \\\n",
    "  --new_model_name \"bge-small-3gpp\" \\\n",
    "  --epochs 10 \\\n",
    "  --batch_size 128 \\\n",
    "  --output_dir \"../models/teleqna/embedding/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef126dff-ef3a-47b5-b9c2-ee66307c70b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Evaluate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1865da05-66b4-4a94-a9c4-b4d7e3504258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelos...\n",
      "Models loaded\n",
      "Loading datasets...\n",
      "Loading and preparing datasets...\n",
      "Datasets loaded and prepared.\n",
      "Loaded dataset\n",
      "Creating evaluator...\n",
      "Evaluator created.\n",
      "Evaluating models\n",
      "Save results...\n"
     ]
    }
   ],
   "source": [
    "!python  ../scripts/evaluate_embedding.py \\\n",
    "  --name_dataset \"teleqna\" \\\n",
    "  --output_dir \"../results/teleqna/\" \\\n",
    "  --models_dir \"../models/teleqna/embedding/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c923fc-55f2-448a-bedc-e85d52b14277",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Create new vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be730a07-012a-48ea-ad16-1bc697ff3f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model embedding\n",
      "Using device: cuda\n",
      "Total number of documents: 554\n",
      "Creating passages\n",
      "  5%|â–ˆâ–‰                                        | 26/554 [00:24<16:45,  1.90s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (832 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 554/554 [10:47<00:00,  1.17s/it]\n",
      "Number of documents: 389606\n",
      "Removing duplicate passages\n",
      "Total number of passages created: 378571\n",
      "Creating vector store\n",
      "Generando embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 185/185 [05:35<00:00,  1.82s/it]\n",
      "âœ… Ãndice FAISS creado exitosamente.\n",
      "ðŸ’¾ Vector store guardado en ../vector_stores/teleqna/ft_vector_store_teleqna\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/create_vector_store.py \\\n",
    "  --dataset \"teleqna\" \\\n",
    "  --emb_model \"../models/teleqna/embedding/bge-small-3gpp_10e_128bs/checkpoint-30\" \\\n",
    "  --cs 150 \\\n",
    "  --co 20 \\\n",
    "  --bs_emb 2048 \\\n",
    "  --output_dir \"../vector_stores/teleqna/ft_\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152e49cc-afb5-46b6-a3de-338bc748c126",
   "metadata": {},
   "source": [
    "## Train Phi-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb71b63b-c2f9-4540-a839-be594027af12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdinho15971\u001b[0m (\u001b[33mdinho15971-unicamp\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/local1/ronaldinho/projects/FinalSBBD/notebooks/wandb/run-20250428_234640-fhs5hsnm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mphi-2-rag-k3_teleqna-5e_10bs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/dinho15971-unicamp/SBBD_phi-2-adapters\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dinho15971-unicamp/SBBD_phi-2-adapters/runs/fhs5hsnm\u001b[0m\n",
      "Usando dispositivo: cuda\n",
      "ðŸ’¾  Vector store cargado desde ../vector_stores/teleqna/ft_vector_store_teleqna\n",
      "Generating dataset\n",
      "Using k = 3 passages\n",
      "Loading and preparing datasets fpr train Phi-2...\n",
      "Train: 724\n",
      "Val: 181\n",
      "Test: 905\n",
      "Datasets loaded and prepared.\n",
      "Creating dataset for teleqna\n",
      "ðŸ” Buscando: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:06<00:00, 14.51it/s]\n",
      "Loading model\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.69it/s]\n",
      "Training\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "ðŸŸ¢ No checkpoints found. Starting training from scratch.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "{'loss': 0.0433, 'grad_norm': 0.2032688558101654, 'learning_rate': 0.0001725995491923131, 'num_tokens': 544504.0, 'mean_token_accuracy': 0.9928199618133073, 'epoch': 1.33}\n",
      "{'loss': 0.0062, 'grad_norm': 0.10751479119062424, 'learning_rate': 9.098469188879349e-05, 'num_tokens': 1075221.0, 'mean_token_accuracy': 0.9984222601369485, 'epoch': 2.66}\n",
      "{'loss': 0.0015, 'grad_norm': 0.007692436221987009, 'learning_rate': 1.6231415137003537e-05, 'num_tokens': 1614544.0, 'mean_token_accuracy': 0.9995706443934097, 'epoch': 3.99}\n",
      "{'train_runtime': 1099.8649, 'train_samples_per_second': 3.291, 'train_steps_per_second': 0.082, 'train_loss': 0.014312266154835622, 'num_tokens': 1933897.0, 'mean_token_accuracy': 0.9998296708391424, 'epoch': 4.77}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [18:19<00:00, 12.22s/it]\n",
      "ParÃ¡metros entrenables finales: 26214400\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: ðŸš€ View run \u001b[33mphi-2-rag-k3_teleqna-5e_10bs\u001b[0m at: \u001b[34mhttps://wandb.ai/dinho15971-unicamp/SBBD_phi-2-adapters/runs/fhs5hsnm\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250428_234640-fhs5hsnm/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python  ../scripts/ft_phi.py \\\n",
    "  --new_model_name \"phi-2-rag-k3_teleqna-5e_10bs\" \\\n",
    "  --num_epochs 5 \\\n",
    "  --batch_size 10 \\\n",
    "  --dataset_name \"teleqna\" \\\n",
    "  --include_docs \\\n",
    "  --top_k 3 \\\n",
    "  --save_path \"../models/teleqna/adapters/\" \\\n",
    "  --vector_store_path \"../vector_stores/teleqna/ft_vector_store_teleqna\" \\\n",
    "  --emb_model \"../models/teleqna/embedding/bge-small-3gpp_10e_128bs/checkpoint-30\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9265ea3f-c56d-4a20-8b71-f86458612cf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdinho15971\u001b[0m (\u001b[33mdinho15971-unicamp\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/local1/ronaldinho/projects/FinalSBBD/notebooks/wandb/run-20250429_190157-h4l74jai\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mphi-2-rag-k3_teleqnaV2-2e_10bs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/dinho15971-unicamp/SBBD_phi-2-adapters\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dinho15971-unicamp/SBBD_phi-2-adapters/runs/h4l74jai\u001b[0m\n",
      "Usando dispositivo: cuda\n",
      "ðŸ’¾  Vector store cargado desde ../vector_stores/teleqna/ft_vector_store_teleqna\n",
      "Generating dataset\n",
      "Using k = 3 passages\n",
      "Loading and preparing datasets fpr train Phi-2...\n",
      "Train: 724\n",
      "Val: 181\n",
      "Test: 905\n",
      "Datasets loaded and prepared.\n",
      "Creating dataset for teleqna\n",
      "ðŸ” Buscando: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:06<00:00, 14.54it/s]\n",
      "Example\n",
      "Instruct: Use the context provided to select the correct option. Select the correct option from A, B, C, D, E. Respond only with the letter of the correct option.\n",
      "Context:\n",
      "\n",
      "Document 0:1>\telse if the failure is detected due to Mobility from NR failure as described in 5.4.3.5, set the fields in VarRLF-report as follows:\n",
      "\n",
      "2>\tset the connectionFailureType to hof;\n",
      "\n",
      "2>\tif last MobilityFromNRCommand concerned a failed inter-RAT handover from NR to E-UTRA and if the UE supports Radio Link Failure Report for Inter-RAT MRO EUTRA (NR to EUTRA):\n",
      "Document 1:Release due to CN-detected mobility\n",
      "\n",
      "The context release is requested by the AMF because the UE is already served by another CN node (same or different system), or another NG interface of the same CN node.\n",
      "\n",
      "N26 interface not available\n",
      "\n",
      "The action failed due to a temporary failure of the N26 interface.\n",
      "\n",
      "Release due to pre-emption\n",
      "\n",
      "Release is initiated due to pre-emption.\n",
      "\n",
      "Multiple Location Reporting Reference ID Instances\n",
      "\n",
      "The action failed because multiple areas of interest are set with the same Location Reporting Reference ID.\n",
      "\n",
      "RSN not available for the UP\n",
      "\n",
      "The redundant user plane resources indicated by RSN are not available.\n",
      "\n",
      "NPN access denied\n",
      "\n",
      "Access was denied, or release is requested, for NPN reasons.\n",
      "\n",
      "CAG only access denied\n",
      "Document 2:\"Relocation failure\" is used by the target MME/S4-SGSN to indicate the source MME/S4-SGSN that the relocation has failed.\n",
      "Document 3:When the UE fails the mobility from NR procedure, it indicates the release of the RRC connection to upper layers together with the release cause 'RRC connection failure'.\n",
      "\n",
      "Question:\n",
      "What release cause is indicated when the UE fails the mobility from NR procedure?\n",
      "\n",
      "Options:\n",
      "A) 'Handover failure'\n",
      "B) 'RRC connection release'\n",
      "C) 'RRC connection failure'\n",
      "D) 'Mobility failure'\n",
      "E) 'Other'\n",
      "\n",
      "Output:\n",
      "C) 'RRC connection failure'\n",
      "Loading model\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.70it/s]\n",
      "Training\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "ðŸŸ¢ No checkpoints found. Starting training from scratch.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "{'loss': 0.0398, 'grad_norm': 0.1176091879606247, 'learning_rate': 5.542616442234618e-05, 'num_tokens': 544504.0, 'mean_token_accuracy': 0.9930831962025043, 'epoch': 1.33}\n",
      "{'train_runtime': 446.8498, 'train_samples_per_second': 3.24, 'train_steps_per_second': 0.081, 'train_loss': 0.030554886700378522, 'num_tokens': 787050.0, 'mean_token_accuracy': 0.9967970130118456, 'epoch': 1.93}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [07:26<00:00, 12.41s/it]\n",
      "ParÃ¡metros entrenables finales: 26214400\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: ðŸš€ View run \u001b[33mphi-2-rag-k3_teleqnaV2-2e_10bs\u001b[0m at: \u001b[34mhttps://wandb.ai/dinho15971-unicamp/SBBD_phi-2-adapters/runs/h4l74jai\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250429_190157-h4l74jai/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python  ../scripts/ft_phi.py \\\n",
    "  --new_model_name \"phi-2-rag-k3_teleqnaV2-2e_10bs\" \\\n",
    "  --num_epochs 2 \\\n",
    "  --batch_size 10 \\\n",
    "  --dataset_name \"teleqna\" \\\n",
    "  --include_docs \\\n",
    "  --top_k 3 \\\n",
    "  --save_path \"../models/teleqna/adapters/\" \\\n",
    "  --vector_store_path \"../vector_stores/teleqna/ft_vector_store_teleqna\" \\\n",
    "  --emb_model \"../models/teleqna/embedding/bge-small-3gpp_10e_128bs/checkpoint-30\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f161ac09-699e-408e-92c2-938a03d27e8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdinho15971\u001b[0m (\u001b[33mdinho15971-unicamp\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/local1/ronaldinho/projects/FinalSBBD/notebooks/wandb/run-20250429_211043-z9x3e90w\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mphi-2-rag-k3_teleqnaV3-2e_10bs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/dinho15971-unicamp/SBBD_phi-2-adapters\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dinho15971-unicamp/SBBD_phi-2-adapters/runs/z9x3e90w\u001b[0m\n",
      "Usando dispositivo: cuda\n",
      "ðŸ’¾  Vector store cargado desde ../vector_stores/teleqna/ft_vector_store_teleqna\n",
      "Generating dataset\n",
      "Using k = 3 passages\n",
      "Loading and preparing datasets fpr train Phi-2...\n",
      "Train: 724\n",
      "Val: 181\n",
      "Test: 905\n",
      "Datasets loaded and prepared.\n",
      "Creating dataset for teleqna\n",
      "ðŸ” Buscando: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:05<00:00, 15.18it/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 724/724 [00:00<00:00, 5522.11 examples/s]\n",
      "Example\n",
      "Instruct: Use the context provided below to answer the question. Select the correct option from A, B, C, D, E. Respond only with the the correct option. Do not include any explanations.\n",
      "Context:\n",
      "\n",
      "Document 0:1>\telse if the failure is detected due to Mobility from NR failure as described in 5.4.3.5, set the fields in VarRLF-report as follows:\n",
      "\n",
      "2>\tset the connectionFailureType to hof;\n",
      "\n",
      "2>\tif last MobilityFromNRCommand concerned a failed inter-RAT handover from NR to E-UTRA and if the UE supports Radio Link Failure Report for Inter-RAT MRO EUTRA (NR to EUTRA):\n",
      "Document 1:Release due to CN-detected mobility\n",
      "\n",
      "The context release is requested by the AMF because the UE is already served by another CN node (same or different system), or another NG interface of the same CN node.\n",
      "\n",
      "N26 interface not available\n",
      "\n",
      "The action failed due to a temporary failure of the N26 interface.\n",
      "\n",
      "Release due to pre-emption\n",
      "\n",
      "Release is initiated due to pre-emption.\n",
      "\n",
      "Multiple Location Reporting Reference ID Instances\n",
      "\n",
      "The action failed because multiple areas of interest are set with the same Location Reporting Reference ID.\n",
      "\n",
      "RSN not available for the UP\n",
      "\n",
      "The redundant user plane resources indicated by RSN are not available.\n",
      "\n",
      "NPN access denied\n",
      "\n",
      "Access was denied, or release is requested, for NPN reasons.\n",
      "\n",
      "CAG only access denied\n",
      "Document 2:\"Relocation failure\" is used by the target MME/S4-SGSN to indicate the source MME/S4-SGSN that the relocation has failed.\n",
      "Document 3:When the UE fails the mobility from NR procedure, it indicates the release of the RRC connection to upper layers together with the release cause 'RRC connection failure'.\n",
      "\n",
      "Question:\n",
      "What release cause is indicated when the UE fails the mobility from NR procedure?\n",
      "\n",
      "Options:\n",
      "A) 'Handover failure'\n",
      "B) 'RRC connection release'\n",
      "C) 'RRC connection failure'\n",
      "D) 'Mobility failure'\n",
      "E) 'Other'\n",
      "\n",
      "Output:\n",
      "C) 'RRC connection failure'\n",
      "Loading model\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.70it/s]\n",
      "Training\n",
      "Converting train dataset to ChatML: 100%|â–ˆ| 724/724 [00:00<00:00, 10954.58 examp\n",
      "Adding EOS to train dataset: 100%|â–ˆâ–ˆ| 724/724 [00:00<00:00, 11642.85 examples/s]\n",
      "Tokenizing train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 724/724 [00:00<00:00, 993.94 examples/s]\n",
      "Truncating train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 724/724 [00:00<00:00, 8320.03 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "ðŸŸ¢ No checkpoints found. Starting training from scratch.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "{'loss': 0.0401, 'grad_norm': 0.11122612655162811, 'learning_rate': 5.542616442234618e-05, 'num_tokens': 548360.0, 'mean_token_accuracy': 0.9940170823913259, 'epoch': 1.33}\n",
      "{'train_runtime': 449.9585, 'train_samples_per_second': 3.218, 'train_steps_per_second': 0.08, 'train_loss': 0.03168864796559016, 'num_tokens': 792666.0, 'mean_token_accuracy': 0.9963152422146364, 'epoch': 1.93}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [07:29<00:00, 12.50s/it]\n",
      "ParÃ¡metros entrenables finales: 26214400\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: ðŸš€ View run \u001b[33mphi-2-rag-k3_teleqnaV3-2e_10bs\u001b[0m at: \u001b[34mhttps://wandb.ai/dinho15971-unicamp/SBBD_phi-2-adapters/runs/z9x3e90w\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250429_211043-z9x3e90w/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python  ../scripts/ft_phi.py \\\n",
    "  --new_model_name \"phi-2-rag-k3_teleqnaV3-2e_10bs\" \\\n",
    "  --num_epochs 2 \\\n",
    "  --batch_size 10 \\\n",
    "  --dataset_name \"teleqna\" \\\n",
    "  --include_docs \\\n",
    "  --top_k 3 \\\n",
    "  --save_path \"../models/teleqna/adapters/\" \\\n",
    "  --vector_store_path \"../vector_stores/teleqna/ft_vector_store_teleqna\" \\\n",
    "  --emb_model \"../models/teleqna/embedding/bge-small-3gpp_10e_128bs/checkpoint-30\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f2dd51f-9c6a-4792-b8d1-a582343965b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdinho15971\u001b[0m (\u001b[33mdinho15971-unicamp\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/local1/ronaldinho/projects/FinalSBBD/notebooks/wandb/run-20250429_213412-gcv6w903\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mphi-2-rag-k3_teleqnaV4-5e_10bs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/dinho15971-unicamp/SBBD_phi-2-adapters\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dinho15971-unicamp/SBBD_phi-2-adapters/runs/gcv6w903\u001b[0m\n",
      "Usando dispositivo: cuda\n",
      "ðŸ’¾  Vector store cargado desde ../vector_stores/teleqna/ft_vector_store_teleqna\n",
      "Generating dataset\n",
      "Using k = 3 passages\n",
      "Loading and preparing datasets fpr train Phi-2...\n",
      "Train: 724\n",
      "Val: 181\n",
      "Test: 905\n",
      "Datasets loaded and prepared.\n",
      "Creating dataset for teleqna\n",
      "ðŸ” Buscando: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:06<00:00, 15.15it/s]\n",
      "Example\n",
      "Instruct: Use the context provided below to answer the question. Select the correct option from A, B, C, D, E. Respond only with the the correct option. Do not include any explanations.\n",
      "Context:\n",
      "\n",
      "Document 0:1>\telse if the failure is detected due to Mobility from NR failure as described in 5.4.3.5, set the fields in VarRLF-report as follows:\n",
      "\n",
      "2>\tset the connectionFailureType to hof;\n",
      "\n",
      "2>\tif last MobilityFromNRCommand concerned a failed inter-RAT handover from NR to E-UTRA and if the UE supports Radio Link Failure Report for Inter-RAT MRO EUTRA (NR to EUTRA):\n",
      "Document 1:Release due to CN-detected mobility\n",
      "\n",
      "The context release is requested by the AMF because the UE is already served by another CN node (same or different system), or another NG interface of the same CN node.\n",
      "\n",
      "N26 interface not available\n",
      "\n",
      "The action failed due to a temporary failure of the N26 interface.\n",
      "\n",
      "Release due to pre-emption\n",
      "\n",
      "Release is initiated due to pre-emption.\n",
      "\n",
      "Multiple Location Reporting Reference ID Instances\n",
      "\n",
      "The action failed because multiple areas of interest are set with the same Location Reporting Reference ID.\n",
      "\n",
      "RSN not available for the UP\n",
      "\n",
      "The redundant user plane resources indicated by RSN are not available.\n",
      "\n",
      "NPN access denied\n",
      "\n",
      "Access was denied, or release is requested, for NPN reasons.\n",
      "\n",
      "CAG only access denied\n",
      "Document 2:\"Relocation failure\" is used by the target MME/S4-SGSN to indicate the source MME/S4-SGSN that the relocation has failed.\n",
      "Document 3:When the UE fails the mobility from NR procedure, it indicates the release of the RRC connection to upper layers together with the release cause 'RRC connection failure'.\n",
      "\n",
      "Question:\n",
      "What release cause is indicated when the UE fails the mobility from NR procedure?\n",
      "\n",
      "Options:\n",
      "A) 'Handover failure'\n",
      "B) 'RRC connection release'\n",
      "C) 'RRC connection failure'\n",
      "D) 'Mobility failure'\n",
      "E) 'Other'\n",
      "\n",
      "Output:\n",
      "C) 'RRC connection failure'\n",
      "Loading model\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.70it/s]\n",
      "Training\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "ðŸŸ¢ No checkpoints found. Starting training from scratch.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "{'loss': 0.0427, 'grad_norm': 0.09812063723802567, 'learning_rate': 0.0001725995491923131, 'num_tokens': 548360.0, 'mean_token_accuracy': 0.9939183986064085, 'epoch': 1.33}\n",
      "{'loss': 0.0073, 'grad_norm': 0.08411668241024017, 'learning_rate': 9.098469188879349e-05, 'num_tokens': 1082933.0, 'mean_token_accuracy': 0.9975587025131147, 'epoch': 2.66}\n",
      "{'loss': 0.0018, 'grad_norm': 0.00952936615794897, 'learning_rate': 1.6231415137003537e-05, 'num_tokens': 1626112.0, 'mean_token_accuracy': 0.9995758263106199, 'epoch': 3.99}\n",
      "{'train_runtime': 1107.548, 'train_samples_per_second': 3.268, 'train_steps_per_second': 0.081, 'train_loss': 0.014674669483469592, 'num_tokens': 1947721.0, 'mean_token_accuracy': 0.9997103245634782, 'epoch': 4.77}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [18:27<00:00, 12.31s/it]\n",
      "ParÃ¡metros entrenables finales: 26214400\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: ðŸš€ View run \u001b[33mphi-2-rag-k3_teleqnaV4-5e_10bs\u001b[0m at: \u001b[34mhttps://wandb.ai/dinho15971-unicamp/SBBD_phi-2-adapters/runs/gcv6w903\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250429_213412-gcv6w903/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python  ../scripts/ft_phi.py \\\n",
    "  --new_model_name \"phi-2-rag-k3_teleqnaV4-5e_10bs\" \\\n",
    "  --num_epochs 5 \\\n",
    "  --batch_size 10 \\\n",
    "  --dataset_name \"teleqna\" \\\n",
    "  --include_docs \\\n",
    "  --top_k 3 \\\n",
    "  --save_path \"../models/teleqna/adapters/\" \\\n",
    "  --vector_store_path \"../vector_stores/teleqna/ft_vector_store_teleqna\" \\\n",
    "  --emb_model \"../models/teleqna/embedding/bge-small-3gpp_10e_128bs/checkpoint-30\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac96d23-f5a1-4920-9f30-8a1813de35ad",
   "metadata": {},
   "source": [
    "## Experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f044e5ac-831c-4972-9046-631da2a19fcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "loading model base\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 18.85it/s]\n",
      "loading adapter\n",
      "ðŸ’¾  Vector store cargado desde ../vector_stores/teleqna/ft_vector_store_teleqna\n",
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
      "Loading and preparing datasets fpr train Phi-2...\n",
      "Train: 724\n",
      "Val: 181\n",
      "Test: 905\n",
      "Datasets loaded and prepared.\n",
      "ðŸ” Buscando: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:04<00:00,  4.33it/s]\n",
      "Generating prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 905/905 [00:00<00:00, 8759.93it/s]\n",
      "Processing inference\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2232 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/inference_rag.py \\\n",
    "  --emb_model \"../models/teleqna/embedding/bge-small-3gpp_10e_128bs/checkpoint-30\" \\\n",
    "  --gen_model \"microsoft/phi-2\" \\\n",
    "  --lora_adapter_path \"../models/teleqna/adapters/finalphi-2-rag-k3_teleqna-5e_10bs\" \\\n",
    "  --max_new_tokens 300 \\\n",
    "  --vector_store_path \"../vector_stores/teleqna/ft_vector_store_teleqna\" \\\n",
    "  --dataset_name \"teleqna\" \\\n",
    "  --output_csv_path \"../results/teleqna/full_ft_teleqna.csv\" \\\n",
    "  --bs_emb 50 \\\n",
    "  --bs_gen 8 \\\n",
    "  --top_k 10 \\\n",
    "  --use_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb6471-eb84-4d90-9bdd-c0e9047c9777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b4f496c-48e0-4100-8b7d-ae9e39550833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "loading model base\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 18.53it/s]\n",
      "loading adapter\n",
      "ðŸ’¾  Vector store cargado desde ../vector_stores/teleqna/ft_vector_store_teleqna\n",
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
      "Loading and preparing datasets fpr train Phi-2...\n",
      "Train: 724\n",
      "Val: 181\n",
      "Test: 905\n",
      "Datasets loaded and prepared.\n",
      "ðŸ” Buscando: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:04<00:00,  4.17it/s]\n",
      "Generating prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 905/905 [00:00<00:00, 9167.27it/s]\n",
      "Processing inference\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2232 > 2048). Running this sequence through the model will result in indexing errors\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/inference_rag.py \\\n",
    "  --emb_model \"../models/teleqna/embedding/bge-small-3gpp_10e_128bs/checkpoint-30\" \\\n",
    "  --gen_model \"microsoft/phi-2\" \\\n",
    "  --lora_adapter_path \"../models/teleqna/adapters/finalphi-2-rag-k3_teleqna-5e_10bs\" \\\n",
    "  --max_new_tokens 30 \\\n",
    "  --vector_store_path \"../vector_stores/teleqna/ft_vector_store_teleqna\" \\\n",
    "  --dataset_name \"teleqna\" \\\n",
    "  --output_csv_path \"../results/teleqna/full_ft_30_teleqna.csv\" \\\n",
    "  --bs_emb 50 \\\n",
    "  --bs_gen 8 \\\n",
    "  --top_k 10 \\\n",
    "  --use_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5066233-70c2-44bb-b092-aae60832f410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "loading model base\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.73it/s]\n",
      "loading adapter\n",
      "ðŸ’¾  Vector store cargado desde ../vector_stores/teleqna/ft_vector_store_teleqna\n",
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
      "Loading and preparing datasets fpr train Phi-2...\n",
      "Train: 724\n",
      "Val: 181\n",
      "Test: 905\n",
      "Datasets loaded and prepared.\n",
      "ðŸ” Buscando: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:04<00:00,  4.20it/s]\n",
      "Generating prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 905/905 [00:00<00:00, 8823.79it/s]\n",
      "Processing inference\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2232 > 2048). Running this sequence through the model will result in indexing errors\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/inference_rag.py \\\n",
    "  --emb_model \"../models/teleqna/embedding/bge-small-3gpp_10e_128bs/checkpoint-30\" \\\n",
    "  --gen_model \"microsoft/phi-2\" \\\n",
    "  --lora_adapter_path \"../models/teleqna/adapters/finalphi-2-rag-k3_teleqnaV2-2e_10bs\" \\\n",
    "  --max_new_tokens 30 \\\n",
    "  --vector_store_path \"../vector_stores/teleqna/ft_vector_store_teleqna\" \\\n",
    "  --dataset_name \"teleqna\" \\\n",
    "  --output_csv_path \"../results/teleqna/full_ftV2_teleqna.csv\" \\\n",
    "  --bs_emb 50 \\\n",
    "  --bs_gen 8 \\\n",
    "  --top_k 10 \\\n",
    "  --use_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86757441-6a73-4def-b378-51f0da59a52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "loading model base\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 18.36it/s]\n",
      "loading adapter\n",
      "ðŸ’¾  Vector store cargado desde ../vector_stores/teleqna/ft_vector_store_teleqna\n",
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
      "Loading and preparing datasets fpr train Phi-2...\n",
      "Train: 724\n",
      "Val: 181\n",
      "Test: 905\n",
      "Datasets loaded and prepared.\n",
      "ðŸ” Buscando: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:04<00:00,  4.36it/s]\n",
      "Generating prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 905/905 [00:00<00:00, 8844.35it/s]\n",
      "Processing inference\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2236 > 2048). Running this sequence through the model will result in indexing errors\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/inference_rag.py \\\n",
    "  --emb_model \"../models/teleqna/embedding/bge-small-3gpp_10e_128bs/checkpoint-30\" \\\n",
    "  --gen_model \"microsoft/phi-2\" \\\n",
    "  --lora_adapter_path \"../models/teleqna/adapters/finalphi-2-rag-k3_teleqnaV3-2e_10bs\" \\\n",
    "  --max_new_tokens 30 \\\n",
    "  --vector_store_path \"../vector_stores/teleqna/ft_vector_store_teleqna\" \\\n",
    "  --dataset_name \"teleqna\" \\\n",
    "  --output_csv_path \"../results/teleqna/full_ftV3_teleqna.csv\" \\\n",
    "  --bs_emb 50 \\\n",
    "  --bs_gen 8 \\\n",
    "  --top_k 10 \\\n",
    "  --use_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2fa1f10-f258-4bcf-84f9-370e52be8c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "loading model base\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 18.83it/s]\n",
      "loading adapter\n",
      "ðŸ’¾  Vector store cargado desde ../vector_stores/teleqna/ft_vector_store_teleqna\n",
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
      "Loading and preparing datasets fpr train Phi-2...\n",
      "Train: 724\n",
      "Val: 181\n",
      "Test: 905\n",
      "Datasets loaded and prepared.\n",
      "ðŸ” Buscando: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:04<00:00,  4.36it/s]\n",
      "Generating prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 905/905 [00:00<00:00, 8952.68it/s]\n",
      "Processing inference\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2236 > 2048). Running this sequence through the model will result in indexing errors\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/inference_rag.py \\\n",
    "  --emb_model \"../models/teleqna/embedding/bge-small-3gpp_10e_128bs/checkpoint-30\" \\\n",
    "  --gen_model \"microsoft/phi-2\" \\\n",
    "  --lora_adapter_path \"../models/teleqna/adapters/finalphi-2-rag-k3_teleqnaV4-5e_10bs\" \\\n",
    "  --max_new_tokens 30 \\\n",
    "  --vector_store_path \"../vector_stores/teleqna/ft_vector_store_teleqna\" \\\n",
    "  --dataset_name \"teleqna\" \\\n",
    "  --output_csv_path \"../results/teleqna/full_ftV4_teleqna.csv\" \\\n",
    "  --bs_emb 50 \\\n",
    "  --bs_gen 8 \\\n",
    "  --top_k 10 \\\n",
    "  --use_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67215d2f-029c-44a0-8000-ca09aa8706d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (env_1)",
   "language": "python",
   "name": "env_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
